# ðŸš€ Pipeline Batch Bovespa

> IngestÃ£o, ETL e consumo de dados do IBOV da B3 na AWS

---

\
\
\


---

## ðŸ“– SumÃ¡rio

1. [VisÃ£o Geral](#visÃ£o-geral)
2. [Arquitetura](#arquitetura)
3. [Componentes](#componentes)
4. [ConfiguraÃ§Ã£o](#configuraÃ§Ã£o)
   - [S3 Buckets](#s3-buckets)
   - [IAM Roles](#iam-roles)
   - [Lambda de IngestÃ£o (Raw)](#lambda-de-ingestÃ£o-raw)
   - [Event Notification](#event-notification)
   - [Lambda Trigger Glue](#lambda-trigger-glue)
   - [Glue Studio â€“ ETL Visual](#glue-studio--etl-visual)
   - [Athena & VisualizaÃ§Ãµes](#athena--visualizaÃ§Ãµes)
   - [Agendamento Opcional](#agendamento-opcional)
5. [Ambiente & VariÃ¡veis](#ambiente--variÃ¡veis)
6. [Queries de Exemplo](#queries-de-exemplo)
7. [Monitoramento](#monitoramento)
8. [PrÃ³ximos Passos](#prÃ³ximos-passos)

---

## VisÃ£o Geral

Este projeto implementa um pipeline completo para:

- **Scraping** dos dados do IBOV (B3)
- Armazenar raw em **Parquet** particionado no **S3**
- Orquestrar via **Lambda â†’ Glue**
- Refinar dados no **Glue Studio (modo visual)**
- Publicar no **Glue Catalog**
- Consumir e visualizar no **Athena**

---

## Arquitetura



1. ðŸŒ **API B3**
2. ðŸŸ¡ **Lambda\_IngestÃ£o** â†’ S3 `raw/`
3. ðŸŸ¡ **Lambda\_Trigger\_Glue** â†’ **Glue Studio**
4. ðŸ”µ **Glue ETL** â†’ S3 `refined/` + **Glue Catalog**
5. ðŸ“— **Athena** (Query & Notebook)

---

## Componentes

| Componente                | FunÃ§Ã£o                                                  |
| ------------------------- | ------------------------------------------------------- |
| **Lambda\_IngestÃ£o**      | Scraping + upload Parquet raw                           |
| **S3 raw/**               | Armazena dados brutos em Parquet particionado por data  |
| **Event Notification**    | Dispara Lambda de trigger ao criar objetos em `raw/`    |
| **Lambda\_Trigger\_Glue** | Inicia o Glue Job de refinamento                        |
| **Glue Studio (ETL)**     | TransformaÃ§Ãµes A, B e C + grava Parquet refinado        |
| **S3 refined/**           | Armazena dados refinados particionados por data e setor |
| **Glue Data Catalog**     | Tabela `default.tb_ibov_refined` criada/atualizada      |
| **AWS Athena**            | Consulta SQL, criaÃ§Ã£o de views e notebooks com grÃ¡ficos |

---

## ConfiguraÃ§Ã£o

### S3 Buckets

```text
tech-challenge-bovespa/
â”œâ”€â”€ raw/       # Parquet raw particionado (date=YYYY-MM-DD)
â””â”€â”€ refined/   # Parquet refinado particionado (date=â€¦/Codigo=â€¦)
```



---

## Lambda de IngestÃ£o (Raw)

**FunÃ§Ã£o:** `lambda_ingestao_bovespa_raw`\
**Runtime:** Python 3.11\
**Layer:** `AWSSDKPandas-Python311:7`

**VariÃ¡veis de Ambiente:**

| Nome           | Valor                    |
| -------------- | ------------------------ |
| `S3_BUCKET`    | `tech-challenge-bovespa` |

**CÃ³digo (**``**):**

```python
import os
import json
import base64
import requests
import pandas as pd
from datetime import datetime
import boto3

# Nome do bucket S3 definido nas variÃ¡veis de ambiente
S3_BUCKET = os.environ.get('S3_BUCKET')
if not S3_BUCKET:
    raise RuntimeError("VariÃ¡vel de ambiente 'S3_BUCKET' nÃ£o definida.")

# Nome do arquivo Parquet a ser gerado
FILENAME = "ibov_setor2.parquet"

def fetch_ibov_por_setor(segment: str = "2", page_size: int = 1000) -> pd.DataFrame:
    """
    Faz scraping paginado dos dados IBOV por setor, retorna DataFrame com:
      - part (float)
      - partAcum (float)
      - theoricalQty (float)
      - DataCarteira (date)
      - demais colunas originais
    """
    page = 1
    all_rows = []
    data_carteira = None

    while True:
        payload = {
            "language": "pt-br",
            "pageNumber": page,
            "pageSize": page_size,
            "index": "IBOV",
            "segment": segment
        }
        # Codifica o payload em Base64 para a chamada da API
        encoded = base64.b64encode(
            json.dumps(payload, ensure_ascii=False).encode("utf-8")
        ).decode("utf-8")
        url = f"https://sistemaswebb3-listados.b3.com.br/indexProxy/indexCall/GetPortfolioDay/{encoded}"
        
        resp = requests.get(url, timeout=15)
        resp.raise_for_status()
        data = resp.json()

        if data_carteira is None:
            date_str = data["header"].get("date")
            if not date_str:
                raise ValueError(f"Header sem campo 'date': {data['header']}")
            data_carteira = datetime.strptime(date_str, "%d/%m/%y").date()

        results = data.get("results", [])
        if not results:
            break

        all_rows.extend(results)
        if len(results) < page_size:
            break
        page += 1

    # ConstrÃ³i DataFrame
    df = pd.DataFrame(all_rows)
    df["DataCarteira"] = data_carteira

    # Converte percentuais (vÃ­rgula â†’ ponto) e transforma em float
    for col in ["part", "partAcum"]:
        if col in df.columns:
            df[col] = df[col].str.replace(",", ".").astype(float)

    # Converte theoricalQty (string com pontos de milhar) para float
    if "theoricalQty" in df.columns:
        df["theoricalQty"] = (
            df["theoricalQty"]
              .astype(str)
              .str.replace(".", "", regex=False)  # remove separador de milhar
              .astype(float)
        )

    return df

def lambda_handler(event, context):
    """
    Handler da AWS Lambda:
      1. Busca dados via fetch_ibov_por_setor()
      2. Gera Parquet particionado por data
      3. Envia o arquivo para o bucket S3
    """
    # 1) Obter DataFrame com os dados
    df = fetch_ibov_por_setor(segment="2", page_size=1000)

    # 2) Definir partiÃ§Ã£o diÃ¡ria no formato date=YYYY-MM-DD
    partition = df["DataCarteira"].iloc[0].strftime("date=%Y-%m-%d")
    s3_key = f"raw/{partition}/{FILENAME}"

    # 3) Salvar localmente e enviar ao S3
    local_path = f"/tmp/{FILENAME}"
    df.to_parquet(local_path, index=False, compression="snappy")

    s3 = boto3.client("s3")
    s3.upload_file(local_path, S3_BUCKET, s3_key)

    # 4) Resposta de sucesso
    return {
        "statusCode": 200,
        "body": json.dumps({
            "message": "Parquet gerado e enviado ao S3 com sucesso.",
            "records": len(df),
            "s3_uri": f"s3://{S3_BUCKET}/{s3_key}"
        })
    }
```

---

## Event Notification

No **S3 â†’ Bucket raw â†’ Properties â†’ Event notifications â†’ Create Event Notification**:

- **Name:** `trigger-lambda-glue-job`
- **Event types:** `All object create events`
- **Prefix:** `raw/`
- **Destination:** Lambda function `lambda_trigger_glue_job`

---

## Lambda Trigger Glue

**FunÃ§Ã£o:** `lambda_trigger_glue_job`\
**Runtime:** Python 3.11

```python
import os
import json
import logging
import boto3

logger = logging.getLogger()
logger.setLevel(logging.INFO)

GLUE_JOB_NAME = os.environ.get("GLUE_JOB_NAME")
if not GLUE_JOB_NAME:
    logger.error("VariÃ¡vel de ambiente 'GLUE_JOB_NAME' nÃ£o definida.")
    raise RuntimeError("VariÃ¡vel de ambiente 'GLUE_JOB_NAME' nÃ£o definida.")

def lambda_handler(event, context):
    try:
        glue = boto3.client("glue")
        response = glue.start_job_run(JobName=GLUE_JOB_NAME)
        job_run_id = response.get("JobRunId")
        logger.info(f"Glue job '{GLUE_JOB_NAME}' iniciado: {job_run_id}")

        return {
            "statusCode": 200,
            "body": json.dumps({
                "message": "Glue job iniciado com sucesso.",
                "job_run_id": job_run_id
            })
        }

    except Exception as e:
        logger.exception("Erro ao iniciar Glue job")
        return {
            "statusCode": 500,
            "body": json.dumps({
                "message": "Falha ao iniciar Glue job.",
                "error": str(e)
            })
        }
```

---

## Glue Studio â€“ ETL Visual

1. **Source**
   - S3 raw (`s3://<bucket>/raw/`), **Recursive**, format **Parquet**, infer schema.
2. **Aggregate**
   - Group by: `segment`, `DataCarteira`
   - Sum(`part`) â†’ `soma_part`
   - Count(`cod`) â†’ `total_codigo`
   - Sum(`theoricalQty`) â†’ `soma_qtd_teorica`
3. **ApplyMapping**
   - `segment` â†’ `Setor`
   - `DataCarteira` â†’ `Data`
   - `soma_part`, `total_codigo`, `soma_qtd_teorica` (tipos ajustados)
4. **SQL Query**
   ```sql
   SELECT
     Setor,
     Data,
     soma_part,
     total_codigo,
     soma_qtd_teorica,
     DATEDIFF(CURRENT_DATE(), Data) AS dias_desde_carteira
   FROM myDataSource;
   ```
5. **Target**
   - S3 refined (`parquet`), partition keys: `Data`, `Codigo`
   - Enable **Create tables in Glue Data Catalog** â†’ Database `default`, Table `tb_ibov_refined`

---

## Athena & VisualizaÃ§Ãµes

### Queries de Exemplo

```sql
-- 10 primeiros registros\SELECT * FROM default.tb_ibov_refined LIMIT 10;

-- Soma total de participaÃ§Ã£o\SELECT SUM(soma_part) AS total_participacao FROM default.tb_ibov_refined;

-- ParticipaÃ§Ã£o por setor
SELECT setor, SUM(soma_part) AS total_part
FROM default.tb_ibov_refined
GROUP BY setor
ORDER BY total_part DESC;
```

### Notebook Spark

```sql
%%sql
SELECT
  Setor,
  SUM(soma_part) AS total_participacao
FROM default.tb_ibov_refined
GROUP BY Setor
ORDER BY total_participacao DESC
LIMIT 10;
```

Clique em **Visualize** (ðŸ“Š) ou use:

```python
df = spark.sql("...sua query...")
pdf = df.toPandas()
import matplotlib.pyplot as plt
plt.figure(figsize=(10,5))
plt.bar(pdf['Setor'], pdf['total_participacao'])
%matplot plt
```

---

